{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN- Bad loan detection w/o region.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kin0969/LendInvest/blob/master/DNN_Bad_loan_detection_w_o_region.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60aj4vAjJEC6",
        "colab_type": "code",
        "outputId": "8db0f948-98ea-46d7-b9f4-9c945a1b5462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import csv\n",
        "\n",
        "# Import DNN Keras framework\n",
        "%tensorflow_version 2.x  #Specific the TF version during Colab update\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers.core import Dense, Dropout, Activation\n",
        "from tensorflow.python.keras.layers import LeakyReLU\n",
        "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D, Flatten,MaxPooling1D\n",
        "from tensorflow.python.keras.optimizers import SGD, Adam\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  #Specific the TF version during Colab update`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM8AIb0oNA26",
        "colab_type": "code",
        "outputId": "ae24bb93-f678-48dc-9e60-4c83b3a41f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# Mount Google Drive for read and write\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "input_x_y_path=\"/content/gdrive/My Drive/Colab Notebooks/LendInvest/DNN/Raw Data/Input X and Y/\"+\"Train_X_Y_20191230_160354.csv\"\n",
        "\n",
        "predict_x_test_path=\"/content/gdrive/My Drive/Colab Notebooks/LendInvest/DNN/Raw Data/Predict_X_test/\"+\"Predict_X_test_2019_09_24.csv\"\n",
        "predict_result_X_test_path=\"/content/gdrive/My Drive/Colab Notebooks/LendInvest/DNN/Result/\"+\"predict_result_X_test.csv\"\n",
        "general_result_path=\"/content/gdrive/My Drive/Colab Notebooks/LendInvest/DNN/Result/\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYNLU1pPM09G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read from Raw Data\n",
        "raw_data=np.genfromtxt((input_x_y_path) ,delimiter=',') \n",
        "X_raw=raw_data[1:,2:8];\n",
        "X_train = np.delete(X_raw, 3, 1)  # delete second column of C\n",
        "Y_train=raw_data[1:,8];\n",
        "\n",
        "# # Fill nan with 0\n",
        "where_nas=np.isnan(X_train)\n",
        "X_train[where_nas]=0\n",
        "\n",
        "# Change numpy array type \n",
        "X_train = X_train.astype('float64') \n",
        "Y_train = Y_train.astype('float64') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRiACNYMO_Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmczeki1OHjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _normalize_column_normal(X, train=True, specified_column = None, X_mean=None, X_std=None):\n",
        "    # The output of the function will make the specified column number to \n",
        "    # become a Normal distribution\n",
        "    # When processing testing data, we need to normalize by the value \n",
        "    # we used for processing training, so we must save the mean value and \n",
        "    # the variance of the training data\n",
        "    if train:\n",
        "        if specified_column == None:\n",
        "            specified_column = np.arange(X.shape[1])\n",
        "        length = len(specified_column)    \n",
        "        X_mean = np.reshape(np.mean(X[:, specified_column],0), (1, length))\n",
        "        X_std  = np.reshape(np.std(X[:, specified_column], 0), (1, length))\n",
        "\n",
        "    X[:,specified_column] = np.divide(np.subtract(X[:,specified_column],X_mean), X_std)\n",
        "    return X, X_mean, X_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uXbLMFeRjDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _shuffle(X, Y):\n",
        "    randomize = np.arange(len(X))\n",
        "    np.random.shuffle(randomize)\n",
        "    return X[randomize], Y[randomize]\n",
        "    \n",
        "def train_dev_split(X, y, dev_size=0.25):    \n",
        "    train_len = int(round(len(X)*(1-dev_size)))\n",
        "    return X[0:train_len,], y[0:train_len,], X[train_len:,], y[train_len:,]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgC1X_NVRx19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(X_train, Y_train):    \n",
        "    dev_size=0.25     #小數點百分比\n",
        "    loss_train=[]  #Base on Cross entropy\n",
        "    loss_dev=[]   #Base on Cross entropy\n",
        "    acc_train=[]\n",
        "    acc_dev=[]  \n",
        "    \n",
        "    # Split a test Set\n",
        "    X_train, Y_train, X_dev, Y_dev=train_dev_split(X_train,Y_train,dev_size)\n",
        "    \n",
        "#     # Transform Y label to 2 column binary format which is for multi-clasification\n",
        "#     from keras.utils import to_categorical\n",
        "#     Y_train = to_categorical(Y_train,2)\n",
        "#     Y_dev = to_categorical(Y_dev,2)\n",
        "    \n",
        "    \n",
        "# from keras.layers import LeakyReLU    \n",
        "    model=Sequential()\n",
        "    model.add(Dense(input_dim=5,units=3,activation='softmax',kernel_regularizer=regularizers.l2(0.01)))\n",
        "    \n",
        "    # model.add(Dense(input_dim=5,units=100,activation='linear',kernel_regularizer=regularizers.l2(0.01)))\n",
        "    # model.add(LeakyReLU(alpha=.005))\n",
        "#     model.add(Dense(input_dim=106,units=666,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
        "#     model.add(Dropout(0.5))\n",
        "    \n",
        "    \n",
        "    for i in range(70):\n",
        "      model.add(Dense(units=3,activation='softmax',kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "#         model.add(Dense(units=666,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "        # model.add(Dense(units=100,activation='linear',kernel_regularizer=regularizers.l2(0.01)))\n",
        "        # model.add(LeakyReLU(alpha=.005))\n",
        "#         model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "        \n",
        "#     which is for multi-clasification\n",
        "#     model.add(Dense(units=2,activation='softmax',kernel_regularizer=regularizers.l2(0.01)))\n",
        "#     model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "   \n",
        "    \n",
        "#     model.add(Dense(units=1,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
        "    \n",
        "\n",
        "    # 最後Layer改成sigmoid 才是2元分類結果\n",
        "    model.add(Dense(units=1,activation='sigmoid',kernel_regularizer=regularizers.l2(0.01)))\n",
        "\n",
        "      \n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "#     model.summary()\n",
        "    \n",
        "    history=model.fit(X_train,Y_train,validation_split=0.000001,batch_size=30,epochs=350)\n",
        "#     history=model.fit(X_train,Y_train,validation_split=0.1155,batch_size=600,epochs=50)\n",
        "    return history,model  # return loss for plotting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lac4Hx3eSZgD",
        "colab_type": "code",
        "outputId": "d75105cd-7735-4d7b-9e7d-101ee43c8695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# These are the columns that I want to normalize\n",
        "col = [0,1,2,3,4]\n",
        "X_train, X_mean, X_std = _normalize_column_normal(X_train, specified_column=col)\n",
        "\n",
        "# return loss is to plot the result\n",
        "history,model= train(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 259 samples, validate on 1 samples\n",
            "Epoch 1/350\n",
            "259/259 [==============================] - 6s 21ms/sample - loss: 2.9012 - accuracy: 0.2664 - val_loss: 2.9046 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/350\n",
            "259/259 [==============================] - 0s 332us/sample - loss: 2.8368 - accuracy: 0.2664 - val_loss: 2.8350 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/350\n",
            "259/259 [==============================] - 0s 288us/sample - loss: 2.7740 - accuracy: 0.2664 - val_loss: 2.7667 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/350\n",
            "259/259 [==============================] - 0s 341us/sample - loss: 2.7128 - accuracy: 0.2664 - val_loss: 2.7000 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/350\n",
            "259/259 [==============================] - 0s 325us/sample - loss: 2.6530 - accuracy: 0.2664 - val_loss: 2.6353 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/350\n",
            "259/259 [==============================] - 0s 322us/sample - loss: 2.5948 - accuracy: 0.2664 - val_loss: 2.5722 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/350\n",
            "259/259 [==============================] - 0s 324us/sample - loss: 2.5380 - accuracy: 0.2664 - val_loss: 2.5109 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/350\n",
            "259/259 [==============================] - 0s 314us/sample - loss: 2.4831 - accuracy: 0.3475 - val_loss: 2.4506 - val_accuracy: 1.0000\n",
            "Epoch 9/350\n",
            "259/259 [==============================] - 0s 302us/sample - loss: 2.4294 - accuracy: 0.7336 - val_loss: 2.3917 - val_accuracy: 1.0000\n",
            "Epoch 10/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 2.3770 - accuracy: 0.7336 - val_loss: 2.3345 - val_accuracy: 1.0000\n",
            "Epoch 11/350\n",
            "259/259 [==============================] - 0s 339us/sample - loss: 2.3260 - accuracy: 0.7336 - val_loss: 2.2789 - val_accuracy: 1.0000\n",
            "Epoch 12/350\n",
            "259/259 [==============================] - 0s 291us/sample - loss: 2.2763 - accuracy: 0.7336 - val_loss: 2.2247 - val_accuracy: 1.0000\n",
            "Epoch 13/350\n",
            "259/259 [==============================] - 0s 293us/sample - loss: 2.2284 - accuracy: 0.7336 - val_loss: 2.1714 - val_accuracy: 1.0000\n",
            "Epoch 14/350\n",
            "259/259 [==============================] - 0s 314us/sample - loss: 2.1814 - accuracy: 0.7336 - val_loss: 2.1196 - val_accuracy: 1.0000\n",
            "Epoch 15/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 2.1355 - accuracy: 0.7336 - val_loss: 2.0692 - val_accuracy: 1.0000\n",
            "Epoch 16/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 2.0910 - accuracy: 0.7336 - val_loss: 2.0205 - val_accuracy: 1.0000\n",
            "Epoch 17/350\n",
            "259/259 [==============================] - 0s 286us/sample - loss: 2.0476 - accuracy: 0.7336 - val_loss: 1.9730 - val_accuracy: 1.0000\n",
            "Epoch 18/350\n",
            "259/259 [==============================] - 0s 330us/sample - loss: 2.0056 - accuracy: 0.7336 - val_loss: 1.9262 - val_accuracy: 1.0000\n",
            "Epoch 19/350\n",
            "259/259 [==============================] - 0s 272us/sample - loss: 1.9645 - accuracy: 0.7336 - val_loss: 1.8805 - val_accuracy: 1.0000\n",
            "Epoch 20/350\n",
            "259/259 [==============================] - 0s 290us/sample - loss: 1.9245 - accuracy: 0.7336 - val_loss: 1.8361 - val_accuracy: 1.0000\n",
            "Epoch 21/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 1.8855 - accuracy: 0.7336 - val_loss: 1.7930 - val_accuracy: 1.0000\n",
            "Epoch 22/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 1.8479 - accuracy: 0.7336 - val_loss: 1.7505 - val_accuracy: 1.0000\n",
            "Epoch 23/350\n",
            "259/259 [==============================] - 0s 273us/sample - loss: 1.8110 - accuracy: 0.7336 - val_loss: 1.7095 - val_accuracy: 1.0000\n",
            "Epoch 24/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 1.7750 - accuracy: 0.7336 - val_loss: 1.6697 - val_accuracy: 1.0000\n",
            "Epoch 25/350\n",
            "259/259 [==============================] - 0s 357us/sample - loss: 1.7402 - accuracy: 0.7336 - val_loss: 1.6308 - val_accuracy: 1.0000\n",
            "Epoch 26/350\n",
            "259/259 [==============================] - 0s 278us/sample - loss: 1.7065 - accuracy: 0.7336 - val_loss: 1.5922 - val_accuracy: 1.0000\n",
            "Epoch 27/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 1.6731 - accuracy: 0.7336 - val_loss: 1.5556 - val_accuracy: 1.0000\n",
            "Epoch 28/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 1.6412 - accuracy: 0.7336 - val_loss: 1.5190 - val_accuracy: 1.0000\n",
            "Epoch 29/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 1.6097 - accuracy: 0.7336 - val_loss: 1.4841 - val_accuracy: 1.0000\n",
            "Epoch 30/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 1.5795 - accuracy: 0.7336 - val_loss: 1.4496 - val_accuracy: 1.0000\n",
            "Epoch 31/350\n",
            "259/259 [==============================] - 0s 255us/sample - loss: 1.5498 - accuracy: 0.7336 - val_loss: 1.4162 - val_accuracy: 1.0000\n",
            "Epoch 32/350\n",
            "259/259 [==============================] - 0s 320us/sample - loss: 1.5212 - accuracy: 0.7336 - val_loss: 1.3836 - val_accuracy: 1.0000\n",
            "Epoch 33/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 1.4932 - accuracy: 0.7336 - val_loss: 1.3521 - val_accuracy: 1.0000\n",
            "Epoch 34/350\n",
            "259/259 [==============================] - 0s 253us/sample - loss: 1.4660 - accuracy: 0.7336 - val_loss: 1.3216 - val_accuracy: 1.0000\n",
            "Epoch 35/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 1.4398 - accuracy: 0.7336 - val_loss: 1.2914 - val_accuracy: 1.0000\n",
            "Epoch 36/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 1.4140 - accuracy: 0.7336 - val_loss: 1.2626 - val_accuracy: 1.0000\n",
            "Epoch 37/350\n",
            "259/259 [==============================] - 0s 287us/sample - loss: 1.3891 - accuracy: 0.7336 - val_loss: 1.2345 - val_accuracy: 1.0000\n",
            "Epoch 38/350\n",
            "259/259 [==============================] - 0s 274us/sample - loss: 1.3648 - accuracy: 0.7336 - val_loss: 1.2073 - val_accuracy: 1.0000\n",
            "Epoch 39/350\n",
            "259/259 [==============================] - 0s 284us/sample - loss: 1.3413 - accuracy: 0.7336 - val_loss: 1.1806 - val_accuracy: 1.0000\n",
            "Epoch 40/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 1.3184 - accuracy: 0.7336 - val_loss: 1.1542 - val_accuracy: 1.0000\n",
            "Epoch 41/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 1.2960 - accuracy: 0.7336 - val_loss: 1.1289 - val_accuracy: 1.0000\n",
            "Epoch 42/350\n",
            "259/259 [==============================] - 0s 298us/sample - loss: 1.2743 - accuracy: 0.7336 - val_loss: 1.1043 - val_accuracy: 1.0000\n",
            "Epoch 43/350\n",
            "259/259 [==============================] - 0s 255us/sample - loss: 1.2532 - accuracy: 0.7336 - val_loss: 1.0808 - val_accuracy: 1.0000\n",
            "Epoch 44/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 1.2329 - accuracy: 0.7336 - val_loss: 1.0572 - val_accuracy: 1.0000\n",
            "Epoch 45/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 1.2130 - accuracy: 0.7336 - val_loss: 1.0341 - val_accuracy: 1.0000\n",
            "Epoch 46/350\n",
            "259/259 [==============================] - 0s 273us/sample - loss: 1.1935 - accuracy: 0.7336 - val_loss: 1.0127 - val_accuracy: 1.0000\n",
            "Epoch 47/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 1.1748 - accuracy: 0.7336 - val_loss: 0.9919 - val_accuracy: 1.0000\n",
            "Epoch 48/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 1.1566 - accuracy: 0.7336 - val_loss: 0.9714 - val_accuracy: 1.0000\n",
            "Epoch 49/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 1.1389 - accuracy: 0.7336 - val_loss: 0.9510 - val_accuracy: 1.0000\n",
            "Epoch 50/350\n",
            "259/259 [==============================] - 0s 274us/sample - loss: 1.1216 - accuracy: 0.7336 - val_loss: 0.9320 - val_accuracy: 1.0000\n",
            "Epoch 51/350\n",
            "259/259 [==============================] - 0s 288us/sample - loss: 1.1050 - accuracy: 0.7336 - val_loss: 0.9129 - val_accuracy: 1.0000\n",
            "Epoch 52/350\n",
            "259/259 [==============================] - 0s 280us/sample - loss: 1.0887 - accuracy: 0.7336 - val_loss: 0.8942 - val_accuracy: 1.0000\n",
            "Epoch 53/350\n",
            "259/259 [==============================] - 0s 269us/sample - loss: 1.0728 - accuracy: 0.7336 - val_loss: 0.8765 - val_accuracy: 1.0000\n",
            "Epoch 54/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 1.0575 - accuracy: 0.7336 - val_loss: 0.8588 - val_accuracy: 1.0000\n",
            "Epoch 55/350\n",
            "259/259 [==============================] - 0s 351us/sample - loss: 1.0425 - accuracy: 0.7336 - val_loss: 0.8423 - val_accuracy: 1.0000\n",
            "Epoch 56/350\n",
            "259/259 [==============================] - 0s 299us/sample - loss: 1.0281 - accuracy: 0.7336 - val_loss: 0.8254 - val_accuracy: 1.0000\n",
            "Epoch 57/350\n",
            "259/259 [==============================] - 0s 289us/sample - loss: 1.0139 - accuracy: 0.7336 - val_loss: 0.8099 - val_accuracy: 1.0000\n",
            "Epoch 58/350\n",
            "259/259 [==============================] - 0s 290us/sample - loss: 1.0005 - accuracy: 0.7336 - val_loss: 0.7932 - val_accuracy: 1.0000\n",
            "Epoch 59/350\n",
            "259/259 [==============================] - 0s 269us/sample - loss: 0.9870 - accuracy: 0.7336 - val_loss: 0.7787 - val_accuracy: 1.0000\n",
            "Epoch 60/350\n",
            "259/259 [==============================] - 0s 274us/sample - loss: 0.9741 - accuracy: 0.7336 - val_loss: 0.7643 - val_accuracy: 1.0000\n",
            "Epoch 61/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.9616 - accuracy: 0.7336 - val_loss: 0.7500 - val_accuracy: 1.0000\n",
            "Epoch 62/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.9495 - accuracy: 0.7336 - val_loss: 0.7359 - val_accuracy: 1.0000\n",
            "Epoch 63/350\n",
            "259/259 [==============================] - 0s 276us/sample - loss: 0.9377 - accuracy: 0.7336 - val_loss: 0.7227 - val_accuracy: 1.0000\n",
            "Epoch 64/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.9262 - accuracy: 0.7336 - val_loss: 0.7101 - val_accuracy: 1.0000\n",
            "Epoch 65/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 0.9151 - accuracy: 0.7336 - val_loss: 0.6981 - val_accuracy: 1.0000\n",
            "Epoch 66/350\n",
            "259/259 [==============================] - 0s 300us/sample - loss: 0.9044 - accuracy: 0.7336 - val_loss: 0.6861 - val_accuracy: 1.0000\n",
            "Epoch 67/350\n",
            "259/259 [==============================] - 0s 295us/sample - loss: 0.8939 - accuracy: 0.7336 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
            "Epoch 68/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.8838 - accuracy: 0.7336 - val_loss: 0.6628 - val_accuracy: 1.0000\n",
            "Epoch 69/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.8739 - accuracy: 0.7336 - val_loss: 0.6516 - val_accuracy: 1.0000\n",
            "Epoch 70/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.8644 - accuracy: 0.7336 - val_loss: 0.6406 - val_accuracy: 1.0000\n",
            "Epoch 71/350\n",
            "259/259 [==============================] - 0s 337us/sample - loss: 0.8551 - accuracy: 0.7336 - val_loss: 0.6308 - val_accuracy: 1.0000\n",
            "Epoch 72/350\n",
            "259/259 [==============================] - 0s 274us/sample - loss: 0.8461 - accuracy: 0.7336 - val_loss: 0.6211 - val_accuracy: 1.0000\n",
            "Epoch 73/350\n",
            "259/259 [==============================] - 0s 269us/sample - loss: 0.8374 - accuracy: 0.7336 - val_loss: 0.6118 - val_accuracy: 1.0000\n",
            "Epoch 74/350\n",
            "259/259 [==============================] - 0s 273us/sample - loss: 0.8290 - accuracy: 0.7336 - val_loss: 0.6022 - val_accuracy: 1.0000\n",
            "Epoch 75/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.8207 - accuracy: 0.7336 - val_loss: 0.5933 - val_accuracy: 1.0000\n",
            "Epoch 76/350\n",
            "259/259 [==============================] - 0s 251us/sample - loss: 0.8128 - accuracy: 0.7336 - val_loss: 0.5842 - val_accuracy: 1.0000\n",
            "Epoch 77/350\n",
            "259/259 [==============================] - 0s 245us/sample - loss: 0.8051 - accuracy: 0.7336 - val_loss: 0.5750 - val_accuracy: 1.0000\n",
            "Epoch 78/350\n",
            "259/259 [==============================] - 0s 251us/sample - loss: 0.7976 - accuracy: 0.7336 - val_loss: 0.5668 - val_accuracy: 1.0000\n",
            "Epoch 79/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.7903 - accuracy: 0.7336 - val_loss: 0.5594 - val_accuracy: 1.0000\n",
            "Epoch 80/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.7833 - accuracy: 0.7336 - val_loss: 0.5517 - val_accuracy: 1.0000\n",
            "Epoch 81/350\n",
            "259/259 [==============================] - 0s 275us/sample - loss: 0.7765 - accuracy: 0.7336 - val_loss: 0.5440 - val_accuracy: 1.0000\n",
            "Epoch 82/350\n",
            "259/259 [==============================] - 0s 255us/sample - loss: 0.7699 - accuracy: 0.7336 - val_loss: 0.5365 - val_accuracy: 1.0000\n",
            "Epoch 83/350\n",
            "259/259 [==============================] - 0s 313us/sample - loss: 0.7635 - accuracy: 0.7336 - val_loss: 0.5294 - val_accuracy: 1.0000\n",
            "Epoch 84/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.7573 - accuracy: 0.7336 - val_loss: 0.5226 - val_accuracy: 1.0000\n",
            "Epoch 85/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.7513 - accuracy: 0.7336 - val_loss: 0.5156 - val_accuracy: 1.0000\n",
            "Epoch 86/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.7455 - accuracy: 0.7336 - val_loss: 0.5088 - val_accuracy: 1.0000\n",
            "Epoch 87/350\n",
            "259/259 [==============================] - 0s 285us/sample - loss: 0.7398 - accuracy: 0.7336 - val_loss: 0.5032 - val_accuracy: 1.0000\n",
            "Epoch 88/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 0.7344 - accuracy: 0.7336 - val_loss: 0.4973 - val_accuracy: 1.0000\n",
            "Epoch 89/350\n",
            "259/259 [==============================] - 0s 243us/sample - loss: 0.7291 - accuracy: 0.7336 - val_loss: 0.4912 - val_accuracy: 1.0000\n",
            "Epoch 90/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.7240 - accuracy: 0.7336 - val_loss: 0.4851 - val_accuracy: 1.0000\n",
            "Epoch 91/350\n",
            "259/259 [==============================] - 0s 303us/sample - loss: 0.7190 - accuracy: 0.7336 - val_loss: 0.4796 - val_accuracy: 1.0000\n",
            "Epoch 92/350\n",
            "259/259 [==============================] - 0s 253us/sample - loss: 0.7142 - accuracy: 0.7336 - val_loss: 0.4741 - val_accuracy: 1.0000\n",
            "Epoch 93/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.7096 - accuracy: 0.7336 - val_loss: 0.4689 - val_accuracy: 1.0000\n",
            "Epoch 94/350\n",
            "259/259 [==============================] - 0s 249us/sample - loss: 0.7051 - accuracy: 0.7336 - val_loss: 0.4631 - val_accuracy: 1.0000\n",
            "Epoch 95/350\n",
            "259/259 [==============================] - 0s 288us/sample - loss: 0.7008 - accuracy: 0.7336 - val_loss: 0.4576 - val_accuracy: 1.0000\n",
            "Epoch 96/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6965 - accuracy: 0.7336 - val_loss: 0.4535 - val_accuracy: 1.0000\n",
            "Epoch 97/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 0.6925 - accuracy: 0.7336 - val_loss: 0.4486 - val_accuracy: 1.0000\n",
            "Epoch 98/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.6885 - accuracy: 0.7336 - val_loss: 0.4445 - val_accuracy: 1.0000\n",
            "Epoch 99/350\n",
            "259/259 [==============================] - 0s 268us/sample - loss: 0.6847 - accuracy: 0.7336 - val_loss: 0.4404 - val_accuracy: 1.0000\n",
            "Epoch 100/350\n",
            "259/259 [==============================] - 0s 257us/sample - loss: 0.6811 - accuracy: 0.7336 - val_loss: 0.4363 - val_accuracy: 1.0000\n",
            "Epoch 101/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.6775 - accuracy: 0.7336 - val_loss: 0.4325 - val_accuracy: 1.0000\n",
            "Epoch 102/350\n",
            "259/259 [==============================] - 0s 245us/sample - loss: 0.6741 - accuracy: 0.7336 - val_loss: 0.4285 - val_accuracy: 1.0000\n",
            "Epoch 103/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.6708 - accuracy: 0.7336 - val_loss: 0.4252 - val_accuracy: 1.0000\n",
            "Epoch 104/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6676 - accuracy: 0.7336 - val_loss: 0.4216 - val_accuracy: 1.0000\n",
            "Epoch 105/350\n",
            "259/259 [==============================] - 0s 269us/sample - loss: 0.6645 - accuracy: 0.7336 - val_loss: 0.4180 - val_accuracy: 1.0000\n",
            "Epoch 106/350\n",
            "259/259 [==============================] - 0s 245us/sample - loss: 0.6616 - accuracy: 0.7336 - val_loss: 0.4139 - val_accuracy: 1.0000\n",
            "Epoch 107/350\n",
            "259/259 [==============================] - 0s 251us/sample - loss: 0.6586 - accuracy: 0.7336 - val_loss: 0.4114 - val_accuracy: 1.0000\n",
            "Epoch 108/350\n",
            "259/259 [==============================] - 0s 278us/sample - loss: 0.6558 - accuracy: 0.7336 - val_loss: 0.4085 - val_accuracy: 1.0000\n",
            "Epoch 109/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6532 - accuracy: 0.7336 - val_loss: 0.4051 - val_accuracy: 1.0000\n",
            "Epoch 110/350\n",
            "259/259 [==============================] - 0s 249us/sample - loss: 0.6506 - accuracy: 0.7336 - val_loss: 0.4022 - val_accuracy: 1.0000\n",
            "Epoch 111/350\n",
            "259/259 [==============================] - 0s 268us/sample - loss: 0.6480 - accuracy: 0.7336 - val_loss: 0.3996 - val_accuracy: 1.0000\n",
            "Epoch 112/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.6456 - accuracy: 0.7336 - val_loss: 0.3970 - val_accuracy: 1.0000\n",
            "Epoch 113/350\n",
            "259/259 [==============================] - 0s 377us/sample - loss: 0.6433 - accuracy: 0.7336 - val_loss: 0.3943 - val_accuracy: 1.0000\n",
            "Epoch 114/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6410 - accuracy: 0.7336 - val_loss: 0.3922 - val_accuracy: 1.0000\n",
            "Epoch 115/350\n",
            "259/259 [==============================] - 0s 246us/sample - loss: 0.6389 - accuracy: 0.7336 - val_loss: 0.3896 - val_accuracy: 1.0000\n",
            "Epoch 116/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.6368 - accuracy: 0.7336 - val_loss: 0.3876 - val_accuracy: 1.0000\n",
            "Epoch 117/350\n",
            "259/259 [==============================] - 0s 261us/sample - loss: 0.6348 - accuracy: 0.7336 - val_loss: 0.3858 - val_accuracy: 1.0000\n",
            "Epoch 118/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.6328 - accuracy: 0.7336 - val_loss: 0.3834 - val_accuracy: 1.0000\n",
            "Epoch 119/350\n",
            "259/259 [==============================] - 0s 245us/sample - loss: 0.6309 - accuracy: 0.7336 - val_loss: 0.3814 - val_accuracy: 1.0000\n",
            "Epoch 120/350\n",
            "259/259 [==============================] - 0s 249us/sample - loss: 0.6291 - accuracy: 0.7336 - val_loss: 0.3786 - val_accuracy: 1.0000\n",
            "Epoch 121/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.6275 - accuracy: 0.7336 - val_loss: 0.3759 - val_accuracy: 1.0000\n",
            "Epoch 122/350\n",
            "259/259 [==============================] - 0s 251us/sample - loss: 0.6257 - accuracy: 0.7336 - val_loss: 0.3747 - val_accuracy: 1.0000\n",
            "Epoch 123/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.6241 - accuracy: 0.7336 - val_loss: 0.3734 - val_accuracy: 1.0000\n",
            "Epoch 124/350\n",
            "259/259 [==============================] - 0s 268us/sample - loss: 0.6225 - accuracy: 0.7336 - val_loss: 0.3722 - val_accuracy: 1.0000\n",
            "Epoch 125/350\n",
            "259/259 [==============================] - 0s 346us/sample - loss: 0.6210 - accuracy: 0.7336 - val_loss: 0.3706 - val_accuracy: 1.0000\n",
            "Epoch 126/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.6195 - accuracy: 0.7336 - val_loss: 0.3687 - val_accuracy: 1.0000\n",
            "Epoch 127/350\n",
            "259/259 [==============================] - 0s 336us/sample - loss: 0.6182 - accuracy: 0.7336 - val_loss: 0.3663 - val_accuracy: 1.0000\n",
            "Epoch 128/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.6168 - accuracy: 0.7336 - val_loss: 0.3648 - val_accuracy: 1.0000\n",
            "Epoch 129/350\n",
            "259/259 [==============================] - 0s 273us/sample - loss: 0.6155 - accuracy: 0.7336 - val_loss: 0.3635 - val_accuracy: 1.0000\n",
            "Epoch 130/350\n",
            "259/259 [==============================] - 0s 272us/sample - loss: 0.6143 - accuracy: 0.7336 - val_loss: 0.3618 - val_accuracy: 1.0000\n",
            "Epoch 131/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 0.6131 - accuracy: 0.7336 - val_loss: 0.3608 - val_accuracy: 1.0000\n",
            "Epoch 132/350\n",
            "259/259 [==============================] - 0s 388us/sample - loss: 0.6119 - accuracy: 0.7336 - val_loss: 0.3600 - val_accuracy: 1.0000\n",
            "Epoch 133/350\n",
            "259/259 [==============================] - 0s 303us/sample - loss: 0.6108 - accuracy: 0.7336 - val_loss: 0.3589 - val_accuracy: 1.0000\n",
            "Epoch 134/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.6098 - accuracy: 0.7336 - val_loss: 0.3580 - val_accuracy: 1.0000\n",
            "Epoch 135/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.6087 - accuracy: 0.7336 - val_loss: 0.3566 - val_accuracy: 1.0000\n",
            "Epoch 136/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 0.6077 - accuracy: 0.7336 - val_loss: 0.3557 - val_accuracy: 1.0000\n",
            "Epoch 137/350\n",
            "259/259 [==============================] - 0s 327us/sample - loss: 0.6068 - accuracy: 0.7336 - val_loss: 0.3549 - val_accuracy: 1.0000\n",
            "Epoch 138/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.6059 - accuracy: 0.7336 - val_loss: 0.3532 - val_accuracy: 1.0000\n",
            "Epoch 139/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.6050 - accuracy: 0.7336 - val_loss: 0.3526 - val_accuracy: 1.0000\n",
            "Epoch 140/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.6042 - accuracy: 0.7336 - val_loss: 0.3512 - val_accuracy: 1.0000\n",
            "Epoch 141/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.6034 - accuracy: 0.7336 - val_loss: 0.3498 - val_accuracy: 1.0000\n",
            "Epoch 142/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6026 - accuracy: 0.7336 - val_loss: 0.3490 - val_accuracy: 1.0000\n",
            "Epoch 143/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.6018 - accuracy: 0.7336 - val_loss: 0.3486 - val_accuracy: 1.0000\n",
            "Epoch 144/350\n",
            "259/259 [==============================] - 0s 261us/sample - loss: 0.6011 - accuracy: 0.7336 - val_loss: 0.3479 - val_accuracy: 1.0000\n",
            "Epoch 145/350\n",
            "259/259 [==============================] - 0s 243us/sample - loss: 0.6004 - accuracy: 0.7336 - val_loss: 0.3471 - val_accuracy: 1.0000\n",
            "Epoch 146/350\n",
            "259/259 [==============================] - 0s 337us/sample - loss: 0.5998 - accuracy: 0.7336 - val_loss: 0.3458 - val_accuracy: 1.0000\n",
            "Epoch 147/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.5991 - accuracy: 0.7336 - val_loss: 0.3451 - val_accuracy: 1.0000\n",
            "Epoch 148/350\n",
            "259/259 [==============================] - 0s 250us/sample - loss: 0.5985 - accuracy: 0.7336 - val_loss: 0.3445 - val_accuracy: 1.0000\n",
            "Epoch 149/350\n",
            "259/259 [==============================] - 0s 247us/sample - loss: 0.5979 - accuracy: 0.7336 - val_loss: 0.3434 - val_accuracy: 1.0000\n",
            "Epoch 150/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.5974 - accuracy: 0.7336 - val_loss: 0.3431 - val_accuracy: 1.0000\n",
            "Epoch 151/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5968 - accuracy: 0.7336 - val_loss: 0.3423 - val_accuracy: 1.0000\n",
            "Epoch 152/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5963 - accuracy: 0.7336 - val_loss: 0.3419 - val_accuracy: 1.0000\n",
            "Epoch 153/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 0.5958 - accuracy: 0.7336 - val_loss: 0.3411 - val_accuracy: 1.0000\n",
            "Epoch 154/350\n",
            "259/259 [==============================] - 0s 310us/sample - loss: 0.5953 - accuracy: 0.7336 - val_loss: 0.3402 - val_accuracy: 1.0000\n",
            "Epoch 155/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.5949 - accuracy: 0.7336 - val_loss: 0.3393 - val_accuracy: 1.0000\n",
            "Epoch 156/350\n",
            "259/259 [==============================] - 0s 239us/sample - loss: 0.5944 - accuracy: 0.7336 - val_loss: 0.3387 - val_accuracy: 1.0000\n",
            "Epoch 157/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.5940 - accuracy: 0.7336 - val_loss: 0.3379 - val_accuracy: 1.0000\n",
            "Epoch 158/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5936 - accuracy: 0.7336 - val_loss: 0.3375 - val_accuracy: 1.0000\n",
            "Epoch 159/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5932 - accuracy: 0.7336 - val_loss: 0.3375 - val_accuracy: 1.0000\n",
            "Epoch 160/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.5928 - accuracy: 0.7336 - val_loss: 0.3371 - val_accuracy: 1.0000\n",
            "Epoch 161/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5925 - accuracy: 0.7336 - val_loss: 0.3364 - val_accuracy: 1.0000\n",
            "Epoch 162/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.5921 - accuracy: 0.7336 - val_loss: 0.3360 - val_accuracy: 1.0000\n",
            "Epoch 163/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5918 - accuracy: 0.7336 - val_loss: 0.3357 - val_accuracy: 1.0000\n",
            "Epoch 164/350\n",
            "259/259 [==============================] - 0s 261us/sample - loss: 0.5915 - accuracy: 0.7336 - val_loss: 0.3363 - val_accuracy: 1.0000\n",
            "Epoch 165/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5912 - accuracy: 0.7336 - val_loss: 0.3353 - val_accuracy: 1.0000\n",
            "Epoch 166/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.5909 - accuracy: 0.7336 - val_loss: 0.3349 - val_accuracy: 1.0000\n",
            "Epoch 167/350\n",
            "259/259 [==============================] - 0s 287us/sample - loss: 0.5906 - accuracy: 0.7336 - val_loss: 0.3347 - val_accuracy: 1.0000\n",
            "Epoch 168/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 0.5904 - accuracy: 0.7336 - val_loss: 0.3346 - val_accuracy: 1.0000\n",
            "Epoch 169/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 0.5901 - accuracy: 0.7336 - val_loss: 0.3342 - val_accuracy: 1.0000\n",
            "Epoch 170/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5899 - accuracy: 0.7336 - val_loss: 0.3343 - val_accuracy: 1.0000\n",
            "Epoch 171/350\n",
            "259/259 [==============================] - 0s 296us/sample - loss: 0.5896 - accuracy: 0.7336 - val_loss: 0.3339 - val_accuracy: 1.0000\n",
            "Epoch 172/350\n",
            "259/259 [==============================] - 0s 326us/sample - loss: 0.5894 - accuracy: 0.7336 - val_loss: 0.3333 - val_accuracy: 1.0000\n",
            "Epoch 173/350\n",
            "259/259 [==============================] - 0s 275us/sample - loss: 0.5892 - accuracy: 0.7336 - val_loss: 0.3327 - val_accuracy: 1.0000\n",
            "Epoch 174/350\n",
            "259/259 [==============================] - 0s 287us/sample - loss: 0.5890 - accuracy: 0.7336 - val_loss: 0.3325 - val_accuracy: 1.0000\n",
            "Epoch 175/350\n",
            "259/259 [==============================] - 0s 272us/sample - loss: 0.5888 - accuracy: 0.7336 - val_loss: 0.3321 - val_accuracy: 1.0000\n",
            "Epoch 176/350\n",
            "259/259 [==============================] - 0s 317us/sample - loss: 0.5886 - accuracy: 0.7336 - val_loss: 0.3320 - val_accuracy: 1.0000\n",
            "Epoch 177/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.5884 - accuracy: 0.7336 - val_loss: 0.3314 - val_accuracy: 1.0000\n",
            "Epoch 178/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5882 - accuracy: 0.7336 - val_loss: 0.3315 - val_accuracy: 1.0000\n",
            "Epoch 179/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5881 - accuracy: 0.7336 - val_loss: 0.3316 - val_accuracy: 1.0000\n",
            "Epoch 180/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 0.5879 - accuracy: 0.7336 - val_loss: 0.3312 - val_accuracy: 1.0000\n",
            "Epoch 181/350\n",
            "259/259 [==============================] - 0s 249us/sample - loss: 0.5877 - accuracy: 0.7336 - val_loss: 0.3306 - val_accuracy: 1.0000\n",
            "Epoch 182/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5876 - accuracy: 0.7336 - val_loss: 0.3304 - val_accuracy: 1.0000\n",
            "Epoch 183/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.5875 - accuracy: 0.7336 - val_loss: 0.3298 - val_accuracy: 1.0000\n",
            "Epoch 184/350\n",
            "259/259 [==============================] - 0s 297us/sample - loss: 0.5873 - accuracy: 0.7336 - val_loss: 0.3295 - val_accuracy: 1.0000\n",
            "Epoch 185/350\n",
            "259/259 [==============================] - 0s 283us/sample - loss: 0.5872 - accuracy: 0.7336 - val_loss: 0.3294 - val_accuracy: 1.0000\n",
            "Epoch 186/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 0.5871 - accuracy: 0.7336 - val_loss: 0.3291 - val_accuracy: 1.0000\n",
            "Epoch 187/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.5870 - accuracy: 0.7336 - val_loss: 0.3285 - val_accuracy: 1.0000\n",
            "Epoch 188/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.5868 - accuracy: 0.7336 - val_loss: 0.3285 - val_accuracy: 1.0000\n",
            "Epoch 189/350\n",
            "259/259 [==============================] - 0s 310us/sample - loss: 0.5867 - accuracy: 0.7336 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "Epoch 190/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5866 - accuracy: 0.7336 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "Epoch 191/350\n",
            "259/259 [==============================] - 0s 273us/sample - loss: 0.5865 - accuracy: 0.7336 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "Epoch 192/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.5864 - accuracy: 0.7336 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
            "Epoch 193/350\n",
            "259/259 [==============================] - 0s 327us/sample - loss: 0.5863 - accuracy: 0.7336 - val_loss: 0.3273 - val_accuracy: 1.0000\n",
            "Epoch 194/350\n",
            "259/259 [==============================] - 0s 328us/sample - loss: 0.5862 - accuracy: 0.7336 - val_loss: 0.3277 - val_accuracy: 1.0000\n",
            "Epoch 195/350\n",
            "259/259 [==============================] - 0s 272us/sample - loss: 0.5861 - accuracy: 0.7336 - val_loss: 0.3277 - val_accuracy: 1.0000\n",
            "Epoch 196/350\n",
            "259/259 [==============================] - 0s 291us/sample - loss: 0.5861 - accuracy: 0.7336 - val_loss: 0.3274 - val_accuracy: 1.0000\n",
            "Epoch 197/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.5860 - accuracy: 0.7336 - val_loss: 0.3279 - val_accuracy: 1.0000\n",
            "Epoch 198/350\n",
            "259/259 [==============================] - 0s 296us/sample - loss: 0.5859 - accuracy: 0.7336 - val_loss: 0.3272 - val_accuracy: 1.0000\n",
            "Epoch 199/350\n",
            "259/259 [==============================] - 0s 275us/sample - loss: 0.5858 - accuracy: 0.7336 - val_loss: 0.3270 - val_accuracy: 1.0000\n",
            "Epoch 200/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.5857 - accuracy: 0.7336 - val_loss: 0.3271 - val_accuracy: 1.0000\n",
            "Epoch 201/350\n",
            "259/259 [==============================] - 0s 287us/sample - loss: 0.5857 - accuracy: 0.7336 - val_loss: 0.3272 - val_accuracy: 1.0000\n",
            "Epoch 202/350\n",
            "259/259 [==============================] - 0s 272us/sample - loss: 0.5856 - accuracy: 0.7336 - val_loss: 0.3269 - val_accuracy: 1.0000\n",
            "Epoch 203/350\n",
            "259/259 [==============================] - 0s 304us/sample - loss: 0.5855 - accuracy: 0.7336 - val_loss: 0.3267 - val_accuracy: 1.0000\n",
            "Epoch 204/350\n",
            "259/259 [==============================] - 0s 283us/sample - loss: 0.5855 - accuracy: 0.7336 - val_loss: 0.3269 - val_accuracy: 1.0000\n",
            "Epoch 205/350\n",
            "259/259 [==============================] - 0s 356us/sample - loss: 0.5854 - accuracy: 0.7336 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "Epoch 206/350\n",
            "259/259 [==============================] - 0s 357us/sample - loss: 0.5854 - accuracy: 0.7336 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "Epoch 207/350\n",
            "259/259 [==============================] - 0s 287us/sample - loss: 0.5853 - accuracy: 0.7336 - val_loss: 0.3261 - val_accuracy: 1.0000\n",
            "Epoch 208/350\n",
            "259/259 [==============================] - 0s 290us/sample - loss: 0.5853 - accuracy: 0.7336 - val_loss: 0.3255 - val_accuracy: 1.0000\n",
            "Epoch 209/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5852 - accuracy: 0.7336 - val_loss: 0.3252 - val_accuracy: 1.0000\n",
            "Epoch 210/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.5851 - accuracy: 0.7336 - val_loss: 0.3254 - val_accuracy: 1.0000\n",
            "Epoch 211/350\n",
            "259/259 [==============================] - 0s 294us/sample - loss: 0.5851 - accuracy: 0.7336 - val_loss: 0.3258 - val_accuracy: 1.0000\n",
            "Epoch 212/350\n",
            "259/259 [==============================] - 0s 296us/sample - loss: 0.5850 - accuracy: 0.7336 - val_loss: 0.3264 - val_accuracy: 1.0000\n",
            "Epoch 213/350\n",
            "259/259 [==============================] - 0s 318us/sample - loss: 0.5850 - accuracy: 0.7336 - val_loss: 0.3264 - val_accuracy: 1.0000\n",
            "Epoch 214/350\n",
            "259/259 [==============================] - 0s 286us/sample - loss: 0.5849 - accuracy: 0.7336 - val_loss: 0.3268 - val_accuracy: 1.0000\n",
            "Epoch 215/350\n",
            "259/259 [==============================] - 0s 318us/sample - loss: 0.5849 - accuracy: 0.7336 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "Epoch 216/350\n",
            "259/259 [==============================] - 0s 277us/sample - loss: 0.5849 - accuracy: 0.7336 - val_loss: 0.3268 - val_accuracy: 1.0000\n",
            "Epoch 217/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5848 - accuracy: 0.7336 - val_loss: 0.3268 - val_accuracy: 1.0000\n",
            "Epoch 218/350\n",
            "259/259 [==============================] - 0s 329us/sample - loss: 0.5848 - accuracy: 0.7336 - val_loss: 0.3270 - val_accuracy: 1.0000\n",
            "Epoch 219/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.5847 - accuracy: 0.7336 - val_loss: 0.3269 - val_accuracy: 1.0000\n",
            "Epoch 220/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5847 - accuracy: 0.7336 - val_loss: 0.3269 - val_accuracy: 1.0000\n",
            "Epoch 221/350\n",
            "259/259 [==============================] - 0s 369us/sample - loss: 0.5847 - accuracy: 0.7336 - val_loss: 0.3272 - val_accuracy: 1.0000\n",
            "Epoch 222/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.5846 - accuracy: 0.7336 - val_loss: 0.3270 - val_accuracy: 1.0000\n",
            "Epoch 223/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5846 - accuracy: 0.7336 - val_loss: 0.3276 - val_accuracy: 1.0000\n",
            "Epoch 224/350\n",
            "259/259 [==============================] - 0s 297us/sample - loss: 0.5846 - accuracy: 0.7336 - val_loss: 0.3279 - val_accuracy: 1.0000\n",
            "Epoch 225/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5845 - accuracy: 0.7336 - val_loss: 0.3276 - val_accuracy: 1.0000\n",
            "Epoch 226/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.5845 - accuracy: 0.7336 - val_loss: 0.3270 - val_accuracy: 1.0000\n",
            "Epoch 227/350\n",
            "259/259 [==============================] - 0s 303us/sample - loss: 0.5844 - accuracy: 0.7336 - val_loss: 0.3273 - val_accuracy: 1.0000\n",
            "Epoch 228/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.5844 - accuracy: 0.7336 - val_loss: 0.3271 - val_accuracy: 1.0000\n",
            "Epoch 229/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.5844 - accuracy: 0.7336 - val_loss: 0.3267 - val_accuracy: 1.0000\n",
            "Epoch 230/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5844 - accuracy: 0.7336 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "Epoch 231/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5843 - accuracy: 0.7336 - val_loss: 0.3264 - val_accuracy: 1.0000\n",
            "Epoch 232/350\n",
            "259/259 [==============================] - 0s 297us/sample - loss: 0.5843 - accuracy: 0.7336 - val_loss: 0.3267 - val_accuracy: 1.0000\n",
            "Epoch 233/350\n",
            "259/259 [==============================] - 0s 247us/sample - loss: 0.5843 - accuracy: 0.7336 - val_loss: 0.3265 - val_accuracy: 1.0000\n",
            "Epoch 234/350\n",
            "259/259 [==============================] - 0s 275us/sample - loss: 0.5842 - accuracy: 0.7336 - val_loss: 0.3263 - val_accuracy: 1.0000\n",
            "Epoch 235/350\n",
            "259/259 [==============================] - 0s 298us/sample - loss: 0.5842 - accuracy: 0.7336 - val_loss: 0.3262 - val_accuracy: 1.0000\n",
            "Epoch 236/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.5842 - accuracy: 0.7336 - val_loss: 0.3258 - val_accuracy: 1.0000\n",
            "Epoch 237/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5842 - accuracy: 0.7336 - val_loss: 0.3256 - val_accuracy: 1.0000\n",
            "Epoch 238/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.5842 - accuracy: 0.7336 - val_loss: 0.3252 - val_accuracy: 1.0000\n",
            "Epoch 239/350\n",
            "259/259 [==============================] - 0s 268us/sample - loss: 0.5841 - accuracy: 0.7336 - val_loss: 0.3255 - val_accuracy: 1.0000\n",
            "Epoch 240/350\n",
            "259/259 [==============================] - 0s 307us/sample - loss: 0.5841 - accuracy: 0.7336 - val_loss: 0.3254 - val_accuracy: 1.0000\n",
            "Epoch 241/350\n",
            "259/259 [==============================] - 0s 266us/sample - loss: 0.5841 - accuracy: 0.7336 - val_loss: 0.3247 - val_accuracy: 1.0000\n",
            "Epoch 242/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5840 - accuracy: 0.7336 - val_loss: 0.3249 - val_accuracy: 1.0000\n",
            "Epoch 243/350\n",
            "259/259 [==============================] - 0s 244us/sample - loss: 0.5840 - accuracy: 0.7336 - val_loss: 0.3250 - val_accuracy: 1.0000\n",
            "Epoch 244/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5840 - accuracy: 0.7336 - val_loss: 0.3245 - val_accuracy: 1.0000\n",
            "Epoch 245/350\n",
            "259/259 [==============================] - 0s 276us/sample - loss: 0.5840 - accuracy: 0.7336 - val_loss: 0.3246 - val_accuracy: 1.0000\n",
            "Epoch 246/350\n",
            "259/259 [==============================] - 0s 242us/sample - loss: 0.5839 - accuracy: 0.7336 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
            "Epoch 247/350\n",
            "259/259 [==============================] - 0s 274us/sample - loss: 0.5839 - accuracy: 0.7336 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
            "Epoch 248/350\n",
            "259/259 [==============================] - 0s 299us/sample - loss: 0.5839 - accuracy: 0.7336 - val_loss: 0.3242 - val_accuracy: 1.0000\n",
            "Epoch 249/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5839 - accuracy: 0.7336 - val_loss: 0.3242 - val_accuracy: 1.0000\n",
            "Epoch 250/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.5839 - accuracy: 0.7336 - val_loss: 0.3243 - val_accuracy: 1.0000\n",
            "Epoch 251/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.5838 - accuracy: 0.7336 - val_loss: 0.3240 - val_accuracy: 1.0000\n",
            "Epoch 252/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.5838 - accuracy: 0.7336 - val_loss: 0.3240 - val_accuracy: 1.0000\n",
            "Epoch 253/350\n",
            "259/259 [==============================] - 0s 289us/sample - loss: 0.5838 - accuracy: 0.7336 - val_loss: 0.3240 - val_accuracy: 1.0000\n",
            "Epoch 254/350\n",
            "259/259 [==============================] - 0s 278us/sample - loss: 0.5838 - accuracy: 0.7336 - val_loss: 0.3236 - val_accuracy: 1.0000\n",
            "Epoch 255/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5837 - accuracy: 0.7336 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "Epoch 256/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 0.5837 - accuracy: 0.7336 - val_loss: 0.3229 - val_accuracy: 1.0000\n",
            "Epoch 257/350\n",
            "259/259 [==============================] - 0s 280us/sample - loss: 0.5837 - accuracy: 0.7336 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "Epoch 258/350\n",
            "259/259 [==============================] - 0s 261us/sample - loss: 0.5837 - accuracy: 0.7336 - val_loss: 0.3228 - val_accuracy: 1.0000\n",
            "Epoch 259/350\n",
            "259/259 [==============================] - 0s 250us/sample - loss: 0.5837 - accuracy: 0.7336 - val_loss: 0.3228 - val_accuracy: 1.0000\n",
            "Epoch 260/350\n",
            "259/259 [==============================] - 0s 255us/sample - loss: 0.5836 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 261/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5836 - accuracy: 0.7336 - val_loss: 0.3223 - val_accuracy: 1.0000\n",
            "Epoch 262/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5836 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 263/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 0.5836 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 264/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.5836 - accuracy: 0.7336 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "Epoch 265/350\n",
            "259/259 [==============================] - 0s 337us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3229 - val_accuracy: 1.0000\n",
            "Epoch 266/350\n",
            "259/259 [==============================] - 0s 258us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "Epoch 267/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3234 - val_accuracy: 1.0000\n",
            "Epoch 268/350\n",
            "259/259 [==============================] - 0s 267us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3228 - val_accuracy: 1.0000\n",
            "Epoch 269/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3229 - val_accuracy: 1.0000\n",
            "Epoch 270/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5835 - accuracy: 0.7336 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "Epoch 271/350\n",
            "259/259 [==============================] - 0s 247us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "Epoch 272/350\n",
            "259/259 [==============================] - 0s 384us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3235 - val_accuracy: 1.0000\n",
            "Epoch 273/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3238 - val_accuracy: 1.0000\n",
            "Epoch 274/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "Epoch 275/350\n",
            "259/259 [==============================] - 0s 324us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3238 - val_accuracy: 1.0000\n",
            "Epoch 276/350\n",
            "259/259 [==============================] - 0s 264us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3234 - val_accuracy: 1.0000\n",
            "Epoch 277/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3248 - val_accuracy: 1.0000\n",
            "Epoch 278/350\n",
            "259/259 [==============================] - 0s 221us/sample - loss: 0.5834 - accuracy: 0.7336 - val_loss: 0.3239 - val_accuracy: 1.0000\n",
            "Epoch 279/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.5833 - accuracy: 0.7336 - val_loss: 0.3242 - val_accuracy: 1.0000\n",
            "Epoch 280/350\n",
            "259/259 [==============================] - 0s 250us/sample - loss: 0.5833 - accuracy: 0.7336 - val_loss: 0.3243 - val_accuracy: 1.0000\n",
            "Epoch 281/350\n",
            "259/259 [==============================] - 0s 243us/sample - loss: 0.5833 - accuracy: 0.7336 - val_loss: 0.3245 - val_accuracy: 1.0000\n",
            "Epoch 282/350\n",
            "259/259 [==============================] - 0s 250us/sample - loss: 0.5833 - accuracy: 0.7336 - val_loss: 0.3254 - val_accuracy: 1.0000\n",
            "Epoch 283/350\n",
            "259/259 [==============================] - 0s 269us/sample - loss: 0.5832 - accuracy: 0.7336 - val_loss: 0.3250 - val_accuracy: 1.0000\n",
            "Epoch 284/350\n",
            "259/259 [==============================] - 0s 249us/sample - loss: 0.5832 - accuracy: 0.7336 - val_loss: 0.3249 - val_accuracy: 1.0000\n",
            "Epoch 285/350\n",
            "259/259 [==============================] - 0s 297us/sample - loss: 0.5832 - accuracy: 0.7336 - val_loss: 0.3244 - val_accuracy: 1.0000\n",
            "Epoch 286/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5832 - accuracy: 0.7336 - val_loss: 0.3236 - val_accuracy: 1.0000\n",
            "Epoch 287/350\n",
            "259/259 [==============================] - 0s 344us/sample - loss: 0.5832 - accuracy: 0.7336 - val_loss: 0.3239 - val_accuracy: 1.0000\n",
            "Epoch 288/350\n",
            "259/259 [==============================] - 0s 278us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3239 - val_accuracy: 1.0000\n",
            "Epoch 289/350\n",
            "259/259 [==============================] - 0s 291us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3238 - val_accuracy: 1.0000\n",
            "Epoch 290/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3239 - val_accuracy: 1.0000\n",
            "Epoch 291/350\n",
            "259/259 [==============================] - 0s 301us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "Epoch 292/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "Epoch 293/350\n",
            "259/259 [==============================] - 0s 241us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3232 - val_accuracy: 1.0000\n",
            "Epoch 294/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3230 - val_accuracy: 1.0000\n",
            "Epoch 295/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.5831 - accuracy: 0.7336 - val_loss: 0.3233 - val_accuracy: 1.0000\n",
            "Epoch 296/350\n",
            "259/259 [==============================] - 0s 254us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3224 - val_accuracy: 1.0000\n",
            "Epoch 297/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 298/350\n",
            "259/259 [==============================] - 0s 291us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3221 - val_accuracy: 1.0000\n",
            "Epoch 299/350\n",
            "259/259 [==============================] - 0s 283us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 300/350\n",
            "259/259 [==============================] - 0s 256us/sample - loss: 0.5830 - accuracy: 0.7336 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "Epoch 301/350\n",
            "259/259 [==============================] - 0s 261us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 302/350\n",
            "259/259 [==============================] - 0s 280us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3224 - val_accuracy: 1.0000\n",
            "Epoch 303/350\n",
            "259/259 [==============================] - 0s 315us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3224 - val_accuracy: 1.0000\n",
            "Epoch 304/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3224 - val_accuracy: 1.0000\n",
            "Epoch 305/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 306/350\n",
            "259/259 [==============================] - 0s 296us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 307/350\n",
            "259/259 [==============================] - 0s 246us/sample - loss: 0.5829 - accuracy: 0.7336 - val_loss: 0.3224 - val_accuracy: 1.0000\n",
            "Epoch 308/350\n",
            "259/259 [==============================] - 0s 247us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 309/350\n",
            "259/259 [==============================] - 0s 243us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3223 - val_accuracy: 1.0000\n",
            "Epoch 310/350\n",
            "259/259 [==============================] - 0s 252us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 311/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "Epoch 312/350\n",
            "259/259 [==============================] - 0s 270us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3214 - val_accuracy: 1.0000\n",
            "Epoch 313/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3209 - val_accuracy: 1.0000\n",
            "Epoch 314/350\n",
            "259/259 [==============================] - 0s 257us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3214 - val_accuracy: 1.0000\n",
            "Epoch 315/350\n",
            "259/259 [==============================] - 0s 322us/sample - loss: 0.5828 - accuracy: 0.7336 - val_loss: 0.3207 - val_accuracy: 1.0000\n",
            "Epoch 316/350\n",
            "259/259 [==============================] - 0s 271us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3208 - val_accuracy: 1.0000\n",
            "Epoch 317/350\n",
            "259/259 [==============================] - 0s 250us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3215 - val_accuracy: 1.0000\n",
            "Epoch 318/350\n",
            "259/259 [==============================] - 0s 275us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "Epoch 319/350\n",
            "259/259 [==============================] - 0s 260us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3219 - val_accuracy: 1.0000\n",
            "Epoch 320/350\n",
            "259/259 [==============================] - 0s 330us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3216 - val_accuracy: 1.0000\n",
            "Epoch 321/350\n",
            "259/259 [==============================] - 0s 265us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "Epoch 322/350\n",
            "259/259 [==============================] - 0s 320us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 323/350\n",
            "259/259 [==============================] - 0s 242us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "Epoch 324/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 325/350\n",
            "259/259 [==============================] - 0s 283us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3228 - val_accuracy: 1.0000\n",
            "Epoch 326/350\n",
            "259/259 [==============================] - 0s 259us/sample - loss: 0.5827 - accuracy: 0.7336 - val_loss: 0.3231 - val_accuracy: 1.0000\n",
            "Epoch 327/350\n",
            "259/259 [==============================] - 0s 240us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 328/350\n",
            "259/259 [==============================] - 0s 257us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 329/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5826 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 330/350\n",
            "259/259 [==============================] - 0s 251us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3221 - val_accuracy: 1.0000\n",
            "Epoch 331/350\n",
            "259/259 [==============================] - 0s 262us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3223 - val_accuracy: 1.0000\n",
            "Epoch 332/350\n",
            "259/259 [==============================] - 0s 244us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 333/350\n",
            "259/259 [==============================] - 0s 263us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3218 - val_accuracy: 1.0000\n",
            "Epoch 334/350\n",
            "259/259 [==============================] - 0s 278us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 335/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3227 - val_accuracy: 1.0000\n",
            "Epoch 336/350\n",
            "259/259 [==============================] - 0s 255us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 337/350\n",
            "259/259 [==============================] - 0s 282us/sample - loss: 0.5825 - accuracy: 0.7336 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 338/350\n",
            "259/259 [==============================] - 0s 281us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3226 - val_accuracy: 1.0000\n",
            "Epoch 339/350\n",
            "259/259 [==============================] - 0s 233us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3222 - val_accuracy: 1.0000\n",
            "Epoch 340/350\n",
            "259/259 [==============================] - 0s 230us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 341/350\n",
            "259/259 [==============================] - 0s 240us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3225 - val_accuracy: 1.0000\n",
            "Epoch 342/350\n",
            "259/259 [==============================] - 0s 244us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3220 - val_accuracy: 1.0000\n",
            "Epoch 343/350\n",
            "259/259 [==============================] - 0s 248us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3219 - val_accuracy: 1.0000\n",
            "Epoch 344/350\n",
            "259/259 [==============================] - 0s 226us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3215 - val_accuracy: 1.0000\n",
            "Epoch 345/350\n",
            "259/259 [==============================] - 0s 240us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3209 - val_accuracy: 1.0000\n",
            "Epoch 346/350\n",
            "259/259 [==============================] - 0s 233us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3216 - val_accuracy: 1.0000\n",
            "Epoch 347/350\n",
            "259/259 [==============================] - 0s 280us/sample - loss: 0.5824 - accuracy: 0.7336 - val_loss: 0.3212 - val_accuracy: 1.0000\n",
            "Epoch 348/350\n",
            "259/259 [==============================] - 0s 279us/sample - loss: 0.5823 - accuracy: 0.7336 - val_loss: 0.3217 - val_accuracy: 1.0000\n",
            "Epoch 349/350\n",
            "259/259 [==============================] - 0s 253us/sample - loss: 0.5823 - accuracy: 0.7336 - val_loss: 0.3211 - val_accuracy: 1.0000\n",
            "Epoch 350/350\n",
            "259/259 [==============================] - 0s 233us/sample - loss: 0.5823 - accuracy: 0.7336 - val_loss: 0.3208 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UalwPbf5Suk9",
        "colab_type": "code",
        "outputId": "3eb346ca-645e-43b3-d094-2e8909a5fdd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "    \n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prx_vT7kh-xh",
        "colab_type": "code",
        "outputId": "7b2b292d-3606-418b-cb1d-f96d31a5b0a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# Predict Test Data\n",
        "# Just for LoanID output\n",
        "X_test_LoanID=np.genfromtxt((predict_x_test_path) ,delimiter=',',dtype=\"U75\") \n",
        "    \n",
        "X_test_data=np.genfromtxt((predict_x_test_path) ,delimiter=',') \n",
        "X_test_raw=X_test_data[1:,2:8];\n",
        "X_test = np.delete(X_test_raw, 3, 1)  # delete second column of C\n",
        "\n",
        "\n",
        "# # Fill nan with 0\n",
        "where_nas=np.isnan(X_test)\n",
        "X_test[where_nas]=0\n",
        "\n",
        "# Change numpy array type \n",
        "X_test = X_test.astype('float64') \n",
        "\n",
        "\n",
        "# Do the same data process to the test data\n",
        "X_test, _, _= _normalize_column_normal(X_test, train=False, specified_column = col, X_mean=X_mean, X_std=X_std)\n",
        "\n",
        "\n",
        "# Predict will give either 0 or 1 as output\n",
        "# Predict_proba will give the only probability of 1.\n",
        "\n",
        "result=model.predict_classes(X_test)\n",
        "print(result)\n",
        "\n",
        "result2=model.predict_proba(X_test)\n",
        "print(result2)\n",
        "\n",
        "\n",
        "# Print Y_dev,Y_dev_pred,y_dev_pred_func_raw     \n",
        "csv_f = open(predict_result_X_test_path,\"w\")\n",
        "csv_w = csv.writer(csv_f)\n",
        "title = ['LoanID','Result_label','Equal to 1 raw']\n",
        "csv_w.writerow(title) \n",
        "for i in range(X_test.shape[0] ):\n",
        "    content = [X_test_LoanID[i+1,1],result[i],result2[i]]\n",
        "    csv_w.writerow(content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]]\n",
            "[[0.7273769]\n",
            " [0.7273769]\n",
            " [0.7273769]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}